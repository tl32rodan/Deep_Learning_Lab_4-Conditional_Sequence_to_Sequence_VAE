{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "from VAE import *\n",
    "from scores import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = load_data('./data/train.txt')\n",
    "test_vocab = load_data('./data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get different tense pairs\n",
    "### !Basically unused in conditional VAE training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tense_paris(train_vocab, input_tense, target_tense):\n",
    "    pairs = []\n",
    "\n",
    "    for vocabs in train_vocab:\n",
    "        pairs.append((vocabs[input_tense],vocabs[target_tense]))\n",
    "        \n",
    "    return pairs  \n",
    "\n",
    "# Simple Present -> Third Person\n",
    "train_st_tp  = get_tense_paris(train_vocab, 0, 1)\n",
    "# Simple Present -> Present Progressive\n",
    "train_st_pp  = get_tense_paris(train_vocab, 0, 2)\n",
    "# Simple Present -> Past\n",
    "train_st_past  = get_tense_paris(train_vocab, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 28 #The number of vocabulary\n",
    "SOS_token = 0\n",
    "EOS_token = vocab_size-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Hyper Parameters----------#\n",
    "hidden_size = 256\n",
    "latent_size = 32\n",
    "teacher_forcing_ratio = 0.6\n",
    "KLD_weight = 0.0\n",
    "lr = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_from_str(target):\n",
    "    ord_a = ord('a')\n",
    "    seq = [ord(c) - ord_a + 1 for c in target]\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def str_from_tensor(target):\n",
    "    seq = ''\n",
    "    for output in target:\n",
    "        _, c = output.topk(1)\n",
    "        seq += chr(c+ord('a')-1)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KL annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_annealing(current_iter, policy = 'mono', reach_max = 25, period = 50):\n",
    "    if policy == 'mono':\n",
    "        beta = 1 if current_iter >= reach_max else current_iter/reach_max\n",
    "    elif policy == 'cyclical':\n",
    "        beta = 1 if current_iter%period >= reach_max else (current_iter%period)/reach_max\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference 4 tense using simple present (for BLEU-4 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_by_simple(vae_model, data_tuple):\n",
    "    pred_tuple = []\n",
    "    \n",
    "    vae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(len(data_tuple)):\n",
    "            input_tense = 0  # Input: simple present\n",
    "            target_tense = i # Target: 4 tense results\n",
    "            input_seq, target_seq = (seq_from_str(data_tuple[input_tense]),seq_from_str(data_tuple[target_tense])) \n",
    "            \n",
    "            # Initialize hidden feature\n",
    "            hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "\n",
    "            result, mu, logvar = vae_model(input_seq, hidden, input_tense, target_tense)\n",
    "            \n",
    "            pred_seq = str_from_tensor(result)\n",
    "            pred_tuple.append(pred_seq[:-1])\n",
    "            \n",
    "    return pred_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_condVAE(vae_model, input_seq, input_cond, target_seq, target_cond, use_teacher_forcing, optimizer, \\\n",
    "                  criterion_CE, criterion_KLD, kl_annealing_beta = 1):    \n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize hidden feature\n",
    "    hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        \n",
    "    # Run model\n",
    "    optimizer.zero_grad()\n",
    "    if use_teacher_forcing:\n",
    "        # input_cond is encoder condition; targer_cond is decoder condition\n",
    "        result, mu, logvar = vae_model(input_seq, hidden, input_cond, target_cond, use_teacher_forcing, target_seq)\n",
    "    else:\n",
    "        result, mu, logvar = vae_model(input_seq, hidden, input_cond, target_cond, use_teacher_forcing, None)\n",
    "            \n",
    "            \n",
    "    # Ground truth should have EOS in the end\n",
    "    target_seq.append(EOS_token)\n",
    "    \n",
    "    # Calculate loss\n",
    "    # First, we should strim the sequences by the length of smaller one\n",
    "    min_len = min(len(target_seq),len(result))\n",
    "        \n",
    "    # hat_y need not to do one-hot encoding\n",
    "    hat_y = result[:min_len]\n",
    "    y = torch.tensor(target_seq[:min_len], device=device)\n",
    "        \n",
    "    ce_loss = criterion_CE(hat_y, y)\n",
    "    kld_loss = criterion_KLD(mu, logvar)\n",
    "    #print('------------------------------')\n",
    "    #print('before: ',kld_loss )\n",
    "    kld_loss = kl_annealing_beta * kld_loss # KL annealing\n",
    "    #print('after: ',kld_loss )\n",
    "    \n",
    "    loss = ce_loss + kld_loss\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return ce_loss.item(), kld_loss.item(), hat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIter_condVAE(vae_model, data, n_epochs, print_every=100, save_every=100, record_every=1,\n",
    "                      learning_rate=0.01, teacher_forcing_ratio = 1.0, \n",
    "                      optimizer = None, scheduler = None,\n",
    "                      criterion_CE = VAE_Loss_CE, criterion_KLD = VAE_Loss_KLD,\n",
    "                      date = '', kl_annealing = 'mono'):\n",
    "    '''\n",
    "        data: A list of 4-tuple\n",
    "              the tense order should be : (simple present, third person, present progressive, past)\n",
    "    '''\n",
    "    loss_list = []\n",
    "    ce_loss_list = []\n",
    "    kld_loss_list = []\n",
    "    bleu_list = []\n",
    "  \n",
    "    # Check optimizer; default: SGD\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.SGD(vae_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(n_epochs): \n",
    "        # Shuffle data\n",
    "        data_tuples = data[:]\n",
    "        random.shuffle(data_tuples)\n",
    "        \n",
    "        avg_bleu = 0.\n",
    "        avg_loss = 0.\n",
    "        avg_ce   = 0.\n",
    "        avg_kld  = 0.\n",
    "        \n",
    "        # KL annealing\n",
    "        beta = KL_annealing(epoch, policy=kl_annealing)\n",
    "        #beta = 1e-4\n",
    "        #print('beta = ',beta)\n",
    "        \n",
    "        for data_tuple in data_tuples:\n",
    "            \n",
    "            # Calculate BLEU-4 score\n",
    "            # Should execute before updating the model\n",
    "            pred = infer_by_simple(vae_model, data_tuple)\n",
    "            avg_bleu += compute_bleu(pred, data_tuple)\n",
    "            \n",
    "            for i in range(4):\n",
    "                for j in range(4):\n",
    "                    input_tense = i # input tense\n",
    "                    target_tense = j# target tense\n",
    "                    input_seq = seq_from_str(data_tuple[input_tense])\n",
    "                    target_seq = seq_from_str(data_tuple[target_tense])                  \n",
    "                    # Determine whether to use teacher forcing\n",
    "                    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "                    # Training\n",
    "                    vae_model.train()\n",
    "                    ce_loss, kld_loss, hat_y = train_condVAE(vae_model, input_seq, input_tense,\\\n",
    "                                                             target_seq, target_tense,\\\n",
    "                                                             use_teacher_forcing, optimizer, \\\n",
    "                                                             criterion_CE, criterion_KLD, beta)\n",
    "                    # Loss\n",
    "                    loss = ce_loss + kld_loss\n",
    "                    #print('loss = ',loss,'; ce_loss = ', ce_loss, '; kld_loss = ',kld_loss)\n",
    "                    avg_loss += loss\n",
    "                    avg_ce   += ce_loss\n",
    "                    avg_kld  += kld_loss                \n",
    "        \n",
    "        \n",
    "        avg_bleu = avg_bleu/(len(data)*4)\n",
    "        avg_loss = avg_loss/(len(data)*4)\n",
    "        avg_ce   = avg_ce/(len(data)*4)\n",
    "        avg_kld  = avg_kld/(len(data)*4)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(avg_loss)\n",
    "        \n",
    "        if (epoch+1) % record_every == 0:\n",
    "            loss_list.append(avg_loss)\n",
    "            ce_loss_list.append(avg_ce)\n",
    "            kld_loss_list.append(avg_kld)\n",
    "            bleu_list.append(avg_bleu)\n",
    "            \n",
    "        if (epoch+1) % print_every == 0:\n",
    "            print('-----------------')\n",
    "            print('Iter %d: avg_loss = %.4f' % (epoch+1, avg_loss))\n",
    "            print('Avg CE = ', avg_ce)\n",
    "            print('Avg KLD = ', avg_kld)\n",
    "            print('Beta = ',beta)\n",
    "            print('Avg BLEU-4 score = ', avg_bleu)\n",
    "            data_tuple = random.choice(data)\n",
    "            pred_seq = infer_by_simple(vae_model, data_tuple)\n",
    "            \n",
    "            print('=========================')\n",
    "            print('|| pred_seq = ', pred_seq)\n",
    "            print('|| target_seq = ', data_tuple)\n",
    "            print('=========================')\n",
    "            \n",
    "        if (epoch+1) % save_every == 0:\n",
    "            torch.save(vae_model,'./models/condVAE_'+str(epoch+1)+date)\n",
    "    \n",
    "    return loss_list, ce_loss_list, kld_loss_list, bleu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vae = CondVAE(vocab_size, hidden_size, vocab_size, teacher_forcing_ratio).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(my_vae.parameters(), lr=lr)\n",
    "lr_sch = optim.lr_scheduler.ReduceLROnPlateau(optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'i' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-da0bf7fd5df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mloss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mce_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkld_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbleu_list\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m             trainIter_condVAE(my_vae, train_vocab, n_epochs=100, \\\n\u001b[0m\u001b[1;32m      3\u001b[0m                            \u001b[0mprint_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_every\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                            \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion_CE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVAE_Loss_CE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-2cc2e190fef4>\u001b[0m in \u001b[0;36mtrainIter_condVAE\u001b[0;34m(vae_model, data, n_epochs, print_every, save_every, record_every, learning_rate, teacher_forcing_ratio, optimizer, scheduler, criterion_CE, criterion_KLD, date, kl_annealing)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# KL annealing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKL_annealing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkl_annealing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m#beta = 1e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m#print('beta = ',beta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'i' referenced before assignment"
     ]
    }
   ],
   "source": [
    "loss_list, ce_loss_list, kld_loss_list, bleu_list = \\\n",
    "            trainIter_condVAE(my_vae, train_vocab, n_epochs=100, \\\n",
    "                           print_every=1, save_every=1, record_every=1,\\\n",
    "                           learning_rate=lr,teacher_forcing_ratio=teacher_forcing_ratio, \\\n",
    "                           optimizer= optimizer, criterion_CE = VAE_Loss_CE, \\\n",
    "                           criterion_KLD = VAE_Loss_KLD,date = '_0814_1530', scheduler = lr_sch,\\\n",
    "                             kl_annealing = 'cyclical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kld_loss_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d2daff533890>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkld_loss_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'kld_loss_list' is not defined"
     ]
    }
   ],
   "source": [
    "plt.plot(kld_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(vae_model, data_pairs, num_eval_data ,criterion_CE = VAE_Loss_CE, criterion_KLD = VAE_Loss_KLD):\n",
    "    loss_list = []\n",
    "    ce_loss_list = []\n",
    "    kld_loss_list = []\n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    vae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_eval_data):\n",
    "            # Seperate pair for input# Randomly generate testing pairs from data\n",
    "            chosen_data = random.choice(data)\n",
    "            input_tense = random.randint(0,3) # Draw input tense\n",
    "            target_tense = random.randint(0,3) # Draw target tense\n",
    "            input_seq, target_seq = (seq_from_str(chosen_data[input_tense]),seq_from_str(chosen_data[target_tense])) \n",
    "            \n",
    "            # Initialize hidden feature\n",
    "            hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "\n",
    "            result, mu, logvar = vae_model(input_seq, hidden, input_tense, target_tense)\n",
    "\n",
    "            # Ground truth should have EOS in the end\n",
    "            target_seq.append(EOS_token)\n",
    "\n",
    "            # Calculate loss\n",
    "            # First, we should strim the sequences by the length of smaller one\n",
    "            min_len = min(len(target_seq),len(result))\n",
    "            hat_y = result[:min_len]\n",
    "            y = torch.tensor(target_seq[:min_len], device=device)\n",
    "\n",
    "            ce_loss = criterion_CE(hat_y, y)\n",
    "            kld_loss = criterion_KLD(mu, logvar)\n",
    "            kld_loss = kld_loss # KL annealing\n",
    "\n",
    "            loss = ce_loss + kld_loss\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            ce_loss_list.append(ce_loss)\n",
    "            kld_loss_list.append(kld_loss)\n",
    "            \n",
    "\n",
    "            # Convert predicted result into str\n",
    "            pred_seq = str_from_tensor(hat_y)\n",
    "            print('-----------------')\n",
    "            print('loss = ', loss)\n",
    "            print('input_seq = ', chosen_data[input_tense])\n",
    "            print('pred_seq = ', pred_seq)\n",
    "            print('target_seq = ', chosen_data[target_tense])\n",
    "            \n",
    "\n",
    "    return loss_list, ce_loss_list, kld_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val(my_vae, train_vocab, num_eval_data= 200, criterion = VAE_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

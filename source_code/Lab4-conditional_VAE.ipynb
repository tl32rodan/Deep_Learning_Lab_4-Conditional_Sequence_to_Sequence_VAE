{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloader import *\n",
    "from VAE import *\n",
    "from scores import *\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = load_data('./data/train.txt')\n",
    "test_vocab = load_data('./data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get different tense pairs\n",
    "### !Basically unused in conditional VAE training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tense_paris(train_vocab, input_tense, target_tense):\n",
    "    pairs = []\n",
    "\n",
    "    for vocabs in train_vocab:\n",
    "        pairs.append((vocabs[input_tense],vocabs[target_tense]))\n",
    "        \n",
    "    return pairs  \n",
    "\n",
    "# Simple Present -> Third Person\n",
    "train_st_tp  = get_tense_paris(train_vocab, 0, 1)\n",
    "# Simple Present -> Present Progressive\n",
    "train_st_pp  = get_tense_paris(train_vocab, 0, 2)\n",
    "# Simple Present -> Past\n",
    "train_st_past  = get_tense_paris(train_vocab, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 28 #The number of vocabulary\n",
    "SOS_token = 0\n",
    "EOS_token = vocab_size-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Hyper Parameters----------#\n",
    "hidden_size = 256\n",
    "latent_size = 64\n",
    "teacher_forcing_ratio = 0.75\n",
    "empty_input_ratio = 0.1\n",
    "KLD_weight = 0.0\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_from_str(target):\n",
    "    ord_a = ord('a')\n",
    "    seq = [ord(c) - ord_a + 1 for c in target]\n",
    "    \n",
    "    return seq\n",
    "\n",
    "def str_from_tensor(target):\n",
    "    seq = ''\n",
    "    for output in target:\n",
    "        _, c = output.topk(1)\n",
    "        seq += chr(c+ord('a')-1)\n",
    "\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use KL annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_annealing(current_iter, policy = 'mono', reach_max = 1000, period = 2000):\n",
    "    if policy == 'mono':\n",
    "        beta = 1 if current_iter >= reach_max else current_iter/reach_max\n",
    "    elif policy == 'cyclical':\n",
    "        beta = 1 if current_iter%period >= reach_max else (current_iter%period)/reach_max\n",
    "    else:\n",
    "        raise ValueError\n",
    "        \n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference 4 tense using simple present (for BLEU-4 score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_by_simple(vae_model, data_tuple):\n",
    "    pred_tuple = []\n",
    "    \n",
    "    vae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(4):\n",
    "            input_tense = 0  # Input: simple present\n",
    "            target_tense = i # Target: 4 tense results\n",
    "            input_seq, target_seq = (seq_from_str(data_tuple[input_tense]),seq_from_str(data_tuple[target_tense])) \n",
    "            \n",
    "            # Initialize hidden feature\n",
    "            hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "\n",
    "            result, mu, logvar = vae_model(input_seq, hidden, input_tense, target_tense)\n",
    "            \n",
    "            pred_seq = str_from_tensor(result)\n",
    "            pred_tuple.append(pred_seq)\n",
    "            \n",
    "    return pred_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_condVAE(vae_model, input_seq, input_cond, target_seq, target_cond, use_teacher_forcing, optimizer, \\\n",
    "                  criterion_CE, criterion_KLD, kl_annealing_beta = 1):    \n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize hidden feature\n",
    "    hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        \n",
    "    # Run model\n",
    "    optimizer.zero_grad()\n",
    "    if use_teacher_forcing:\n",
    "        # input_cond is encoder condition; targer_cond is decoder condition\n",
    "        result, mu, logvar = vae_model(input_seq, hidden, input_cond, target_cond, use_teacher_forcing, target_seq)\n",
    "    else:\n",
    "        result, mu, logvar = vae_model(input_seq, hidden, input_cond, target_cond, use_teacher_forcing, None)\n",
    "            \n",
    "            \n",
    "    # Ground truth should have EOS in the end\n",
    "    target_seq.append(EOS_token)\n",
    "    \n",
    "    # Calculate loss\n",
    "    # First, we should strim the sequences by the length of smaller one\n",
    "    min_len = min(len(target_seq),len(result))\n",
    "        \n",
    "    # hat_y need not to do one-hot encoding\n",
    "    hat_y = result[:min_len]\n",
    "    y = torch.tensor(target_seq[:min_len], device=device)\n",
    "        \n",
    "    ce_loss = criterion_CE(hat_y, y)\n",
    "    kld_loss = criterion_KLD(mu, logvar)\n",
    "    kld_loss = kl_annealing_beta * kld_loss # KL annealing\n",
    "    \n",
    "    loss = ce_loss + kld_loss\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return ce_loss.item(), kld_loss.item(), hat_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIter_condVAE(vae_model, data, n_iters, print_every=1000, save_every=1000, record_every=1000,\n",
    "                      learning_rate=0.01, teacher_forcing_ratio = 1.0, \n",
    "                      optimizer = None, scheduler = None,\n",
    "                      criterion_CE = VAE_Loss_CE, criterion_KLD = VAE_Loss_KLD,\n",
    "                      date = '', kl_annealing = 'mono'):\n",
    "    '''\n",
    "        data: A list of 4-tuple\n",
    "              the tense order should be : (simple present, third person, present progressive, past)\n",
    "    '''\n",
    "    loss_list = []\n",
    "    ce_loss_list = []\n",
    "    kld_loss_list = []\n",
    "    bleu_list = []\n",
    "  \n",
    "    # Check optimizer; default: SGD\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.SGD(vae_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for i in range(n_iters): \n",
    "        vae_model.train()\n",
    "        \n",
    "        # Randomly generate training pairs from data\n",
    "        chosen_data = random.choice(data)\n",
    "        input_tense = random.randint(0,3) # Draw input tense\n",
    "        #target_tense = random.randint(0,3) # Draw target tense\n",
    "        target_tense = input_tense\n",
    "        input_seq = seq_from_str(chosen_data[input_tense])\n",
    "        target_seq = seq_from_str(chosen_data[target_tense])                  \n",
    "        \n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        \n",
    "        # KL annealing's beta\n",
    "        beta = KL_annealing(i, policy=kl_annealing)\n",
    "        ce_loss, kld_loss, hat_y = train_condVAE(vae_model, input_seq, input_tense, target_seq, target_tense,\\\n",
    "                             use_teacher_forcing, optimizer, criterion_CE, criterion_KLD, beta)\n",
    "    \n",
    "        # Loss\n",
    "        loss = ce_loss + kld_loss\n",
    "        \n",
    "        if (i+1) % record_every == 0 or (i+1) % print_every == 0:\n",
    "            # BLEU-4 score\n",
    "            pred = infer_by_simple(vae_model, chosen_data)\n",
    "            bleu_score = compute_bleu(pred, chosen_data)\n",
    "        \n",
    "        # Convert output to str\n",
    "        pred_seq = str_from_tensor(hat_y)\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(loss)\n",
    "        \n",
    "        if (i+1) % record_every == 0:\n",
    "            loss_list.append(loss)\n",
    "            ce_loss_list.append(ce_loss)\n",
    "            kld_loss_list.append(kld_loss)\n",
    "            bleu_list.append(bleu_score)\n",
    "            \n",
    "        if (i+1) % print_every == 0:\n",
    "            print('-----------------')\n",
    "            print('Iter %d: loss = %.4f' % (i+1, loss))\n",
    "            print('ce_loss = ', ce_loss)\n",
    "            print('kld_loss = ', kld_loss)\n",
    "            print('    ==================')\n",
    "            print('    pred = ', pred)\n",
    "            print('    chosen_data = ', chosen_data)\n",
    "            print('    BLEU-4 score = ', bleu_score)\n",
    "            print('    ==================')\n",
    "            print('input_seq = ', chosen_data[input_tense])\n",
    "            print('pred_seq = ', pred_seq)\n",
    "            print('target_seq = ', chosen_data[target_tense])\n",
    "            \n",
    "        if (i+1) % save_every == 0:\n",
    "            torch.save(vae_model,'./models/condVAE_'+str(i+1)+date)\n",
    "    \n",
    "    return loss_list, ce_loss_list, kld_loss_list, bleu_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vae = CondVAE(vocab_size, hidden_size, vocab_size, teacher_forcing_ratio).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(my_vae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_list, ce_loss_list, kld_loss_list, bleu_list = \\\n",
    "            trainIter_condVAE(my_vae, train_vocab, n_iters=10000, \\\n",
    "                           print_every=1000, save_every=100000, record_every=200,\\\n",
    "                           learning_rate=lr,teacher_forcing_ratio=teacher_forcing_ratio, \\\n",
    "                           optimizer= optimizer, criterion_CE = VAE_Loss_CE, \\\n",
    "                           criterion_KLD = VAE_Loss_KLD,date = '_0813_1950')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(vae_model, data_pairs, num_eval_data ,criterion_CE = VAE_Loss_CE, criterion_KLD = VAE_Loss_KLD):\n",
    "    loss_list = []\n",
    "    ce_loss_list = []\n",
    "    kld_loss_list = []\n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    vae_model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(num_eval_data):\n",
    "            # Seperate pair for input# Randomly generate testing pairs from data\n",
    "            chosen_data = random.choice(data)\n",
    "            input_tense = random.randint(0,3) # Draw input tense\n",
    "            target_tense = random.randint(0,3) # Draw target tense\n",
    "            input_seq, target_seq = (seq_from_str(chosen_data[input_tense]),seq_from_str(chosen_data[target_tense])) \n",
    "            \n",
    "            # Initialize hidden feature\n",
    "            hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "\n",
    "            result, mu, logvar = vae_model(input_seq, hidden, input_tense, target_tense)\n",
    "\n",
    "            # Ground truth should have EOS in the end\n",
    "            target_seq.append(EOS_token)\n",
    "\n",
    "            # Calculate loss\n",
    "            # First, we should strim the sequences by the length of smaller one\n",
    "            min_len = min(len(target_seq),len(result))\n",
    "            hat_y = result[:min_len]\n",
    "            y = torch.tensor(target_seq[:min_len], device=device)\n",
    "\n",
    "            ce_loss = criterion_CE(hat_y, y)\n",
    "            kld_loss = criterion_KLD(mu, logvar)\n",
    "            kld_loss = kld_loss # KL annealing\n",
    "\n",
    "            loss = ce_loss + kld_loss\n",
    "            \n",
    "            loss_list.append(loss)\n",
    "            ce_loss_list.append(ce_loss)\n",
    "            kld_loss_list.append(kld_loss)\n",
    "            \n",
    "\n",
    "            # Convert predicted result into str\n",
    "            pred_seq = str_from_tensor(hat_y)\n",
    "            print('-----------------')\n",
    "            print('loss = ', loss)\n",
    "            print('input_seq = ', chosen_data[input_tense])\n",
    "            print('pred_seq = ', pred_seq)\n",
    "            print('target_seq = ', chosen_data[target_tense])\n",
    "            \n",
    "\n",
    "    return loss_list, ce_loss_list, kld_loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val(my_vae, train_vocab, num_eval_data= 200, criterion = VAE_Loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

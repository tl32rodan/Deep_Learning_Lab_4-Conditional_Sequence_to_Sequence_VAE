{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "from os import system\n",
    "from dataloader import *\n",
    "from VAE import *\n",
    "from scores import *\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = load_data('./data/train.txt')\n",
    "test_vocab = load_data('./data/test.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get different tense pairs for (unconditional) VAE training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tense_paris(train_vocab, source_index, target_index):\n",
    "    pairs = []\n",
    "\n",
    "    for vocabs in train_vocab:\n",
    "        pairs.append((vocabs[source_index],vocabs[target_index]))\n",
    "        \n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Present -> Third Person"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st_tp  = get_tense_paris(train_vocab, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Present -> Present Progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st_pp  = get_tense_paris(train_vocab, 0, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Present -> Past"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_st_past  = get_tense_paris(train_vocab, 0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 28 #The number of vocabulary\n",
    "SOS_token = 0\n",
    "EOS_token = vocab_size-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------Hyper Parameters----------#\n",
    "hidden_size = 256\n",
    "teacher_forcing_ratio = 1.0\n",
    "empty_input_ratio = 0.1\n",
    "KLD_weight = 0.0\n",
    "lr = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqFromPair(pair):\n",
    "    ord_a = ord('a')\n",
    "    input_seq = [ord(c) - ord_a + 1 for c in pair[0]]\n",
    "    target_seq = [ord(c) - ord_a + 1 for c in pair[1]]\n",
    "    \n",
    "    return input_seq, target_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(vae_model, input_seq, target_seq, use_teacher_forcing, optimizer, criterion):\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Check device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Initialize hidden feature\n",
    "    hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        \n",
    "    # Run model\n",
    "    if use_teacher_forcing:\n",
    "        result, mu, logvar = vae_model(input_seq, hidden, use_teacher_forcing, target_seq)\n",
    "    else:\n",
    "        result, mu, logvar = vae_model(input_seq, hidden, use_teacher_forcing, None)\n",
    "            \n",
    "            \n",
    "    # Ground truth should have EOS in the end\n",
    "    target_seq.append(EOS_token)\n",
    "        \n",
    "    # Calculate loss\n",
    "    # First, we should strim the sequences by the length of smaller one\n",
    "    min_len = min(len(target_seq),len(result))\n",
    "        \n",
    "    # hat_y need not to do one-hot encoding\n",
    "    hat_y = result[:min_len]\n",
    "    y = torch.tensor(target_seq[:min_len], device=device)\n",
    "        \n",
    "    loss = criterion(hat_y, y, mu, logvar)\n",
    "        \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIter(vae_model, data_pairs, n_iters, print_every=1000, learning_rate=0.01, teacher_forcing_ratio = 1.0,\\\n",
    "         optimizer = None, criterion = VAE_Loss):\n",
    "    loss_list = []\n",
    "    \n",
    "    # Randomly generate training pairs from data\n",
    "    training_pairs = [seqFromPair(random.choice(data_pairs))\n",
    "                      for i in range(n_iters)]    \n",
    "    \n",
    "    # Check optimizer; default: SGD\n",
    "    if optimizer is None:\n",
    "        optimizer = optim.SGD(vae_model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    vae_model.train()\n",
    "    for i in range(n_iters):        \n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "        # Seperate pair for input\n",
    "        input_seq, target_seq = training_pairs[i] \n",
    "        \n",
    "        loss = train(vae_model, input_seq, target_seq, use_teacher_forcing, optimizer, criterion)\n",
    "    \n",
    "        loss_list.append(loss)\n",
    "        if (i+1) % print_every == 0:\n",
    "            print('Iter %d: loss = %.4f' % (i+1, loss))\n",
    "    \n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_vae = VAE(vocab_size, hidden_size, vocab_size, teacher_forcing_ratio).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(my_vae.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train with Simple Present -> Present Progressive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "loss_list = trainIter(my_vae, train_st_pp, n_iters=100000, print_every=500, learning_rate=lr, \\\n",
    "      teacher_forcing_ratio=teacher_forcing_ratio, optimizer= optimizer, criterion = VAE_Loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val(vae_model, data_pairs, criterion = VAE_Loss):\n",
    "    loss_list = []\n",
    "    \n",
    "    vae_model.eval()\n",
    "    for data_pair in data_pairs:\n",
    "        # Seperate pair for input\n",
    "        pair = seqFromPair(data_pair)\n",
    "        input_seq, target_seq = pair\n",
    "        \n",
    "        # Check device\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Initialize hidden feature\n",
    "        hidden = torch.zeros(1, 1, hidden_size, device=device)\n",
    "        \n",
    "        result, mu, logvar = vae_model(input_seq, hidden)\n",
    "        \n",
    "        # Ground truth should have EOS in the end\n",
    "        target_seq.append(EOS_token)\n",
    "\n",
    "        # Calculate loss\n",
    "        # First, we should strim the sequences by the length of smaller one\n",
    "        min_len = min(len(target_seq),len(result))\n",
    "\n",
    "        # hat_y need not to do one-hot encoding\n",
    "        hat_y = result[:min_len]\n",
    "        y = torch.tensor(target_seq[:min_len], device=device)\n",
    "\n",
    "        loss = criterion(hat_y, y, mu, logvar)\n",
    "        \n",
    "        pred_seq = ''\n",
    "        for output in hat_y:\n",
    "            _, c = output.topk(1)\n",
    "            pred_seq += chr(c+ord('a')-1)\n",
    "        print('-----------------')\n",
    "        print('loss = ', loss)\n",
    "        print('pred_seq = ', pred_seq)\n",
    "        print('target_seq = ', data_pair[1][:min_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val(my_vae, train_st_pp, criterion = VAE_Loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
